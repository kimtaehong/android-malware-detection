from os.path import join, dirname, realpath, basename
from pickle import loads
from csv import writer, QUOTE_MINIMAL
from optparse import OptionParser

from multiprocessing import Queue, Pool, cpu_count

from androguard.misc import *

from log import log

import pickle
import glob


data_queue = Queue()

# load classifiers

with open('classifier/apicall.pkl', 'rb') as f:
    apicall = pickle.load(f)


with open('classifier/strings.pkl', 'rb') as f:
    suspicious_strings = pickle.load(f)


def analyze_apk_file(apk_file_path):
    """Analyze APK file."""
    a, dexs, analysis = AnalyzeAPK(apk_file_path)
    return a.get_permissions(), dexs, analysis


def extract_feature(file):
    global apicall, suspicious_strings
    file_name = basename(file)
    log().info("analyzing file: {}".format(file_name))
    permissions, dexs, analysis = analyze_apk_file(file)
    res = {}

    for feature in features:
        res.setdefault(feature, 0)

    # permissions
    for permission in permissions:
        if permission in res:
            res[permission] = 1

    # method call
    methods = analysis.get_methods()
    call_count = {}

    for method in methods:
        if method.is_android_api():
            info = method.get_method()
            name = "{}.{}".format(info.get_class_name()[1:-1].replace('/', '.'), info.name)
            if name in apicall:
                for permission in apicall[name]:
                    if permission in call_count:
                        call_count[permission] += 1
                    else:
                        call_count[permission] = 1

    for key in call_count.keys():
        count = call_count[key]
        name = "{}/method".format(key)
        res[name] = count

    # string sections
    suspicious_string_count = {
        'http': 0,
        'https': 0,
    }

    for dex in dexs:
        for string in dex.get_strings():
            if string in suspicious_strings:
                if string in suspicious_string_count:
                    suspicious_string_count[string] += 1
                else:
                    suspicious_string_count[string] = 1

            # http with startswith
            if string.startswith('http://'):
                suspicious_string_count['http'] += 1

            elif string.startswith('https://'):
                suspicious_string_count['https'] += 1

    for key in suspicious_string_count.keys():
        count = suspicious_string_count[key]
        name = "{}/suspicious_strings".format(key)
        res[name] = count

    # make feature tables
    p = [0] * len(features)
    for idx in range(len(features)):
        p[idx] = res[features[idx]]

    log().info("Successfully extract feature from {} file".format(file_name))

    return p, file_name


def process_file(data):
    global data_queue
    file, file_type = data
    p, file_name = extract_feature(file)
    p.insert(0, file_name)
    p.insert(1, file_type)

    data_queue.put(p)


if __name__ == '__main__':
    opt_parser = OptionParser()
    opt_parser.set_defaults(inmemory=False, debug=False, UseLocalTimezone=True, UseGUI=False)
    opt_parser.add_option('-t', '--train-data-path',
                          dest='train_data_path',
                          help='Input train data path')

    options, args = opt_parser.parse_args()
    if options.train_data_path is None:
        opt_parser.print_help()
        exit(-1)

    current_module_path = dirname(realpath(__file__))
    with open(join(current_module_path, 'classifier/permissions.pkl'), 'rb') as f:
        features = loads(f.read())

    with open(join(current_module_path, 'classifier/apicall_header.pkl'), 'rb') as f:
        features += loads(f.read())

    with open(join(current_module_path, 'classifier/strings_header.pkl'), 'rb') as f:
        features += loads(f.read())

    log().info('load {} features'.format(len(features)))

    # set train csv file
    csv_file_object = open('train.csv', 'w')
    csv_writer = writer(csv_file_object, delimiter=',', quoting=QUOTE_MINIMAL)

    csv_headers = features.copy()
    csv_headers.insert(0, 'name')
    csv_headers.insert(1, 'type')

    csv_writer.writerow(csv_headers)

    malware_data_path = join(options.train_data_path, '1-malware')

    pool = Pool(processes=cpu_count() * 2)
    pool.map(
        process_file,
        [[filepath, 1] for filepath in glob.glob('{}/*.vir'.format(malware_data_path))])

    normal_data_path = join(options.train_data_path, '0-normal')

    pool.map(
        process_file,
        [[filepath, 0] for filepath in glob.glob('{}/*.vir'.format(normal_data_path))])

    while not data_queue.empty():
        csv_writer.writerow(data_queue.get())

    log().info('Successfully done:-')
    csv_file_object.close()
